{"cells":[{"cell_type":"markdown","metadata":{"id":"q378C5GQfJS5"},"source":["## Import necessary libraries & set up the environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dbImUIMwfgz-"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1r_55jOogPZ3"},"outputs":[],"source":["!pip install transformers datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bouonw7SfJS7"},"outputs":[],"source":["import os\n","from copy import deepcopy\n","from dataclasses import dataclass\n","from typing import Dict, List, Optional, Tuple\n","from datasets import load_dataset, set_caching_enabled\n","import numpy as np\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","from transformers import (\n","    # Preprocessing / Common\n","    AutoTokenizer, AutoFeatureExtractor,\n","    # Text & Image Models (Now, image transformers like ViTModel, DeiTModel, BEiT can also be loaded using AutoModel)\n","    AutoModel,            \n","    # Training / Evaluation\n","    TrainingArguments, Trainer,\n","    # Misc\n","    logging\n",")\n","\n","# import nltk\n","# nltk.download('wordnet')\n","from nltk.corpus import wordnet\n","\n","from sklearn.metrics import accuracy_score, f1_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fMZ2xUcxfJS-"},"outputs":[],"source":["# SET CACHE FOR HUGGINGFACE TRANSFORMERS + DATASETS\n","os.environ['HF_HOME'] = os.path.join(\".\", \"cache\")\n","# SET ONLY 1 GPU DEVICE\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","\n","set_caching_enabled(True)\n","logging.set_verbosity_error()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iEBs4DnvfJTA"},"outputs":[],"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","#Additional Info when using cuda\n","if device.type == 'cuda':\n","    print(torch.cuda.get_device_name(0))\n","    print('Memory Usage:')\n","    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"]},{"cell_type":"markdown","metadata":{"id":"n9IrJW-rfJTD"},"source":["## Load the Processed [DAQUAR Dataset](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/vision-and-language/visual-turing-challenge/)"]},{"cell_type":"markdown","metadata":{"id":"fWtbpiVffJTE"},"source":["All the questions have 1-word/phrase answer, so we consider the entire vocabulary of answers available (*answer space*) & treat them as labels. This converts the visual question answering into a multi-class classification problem."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pfYGb0mMIA-w"},"outputs":[],"source":["!unzip /content/drive/MyDrive/vivqa_on_book/data/book_cover_img.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PN_imflV2L9L"},"outputs":[],"source":["!python /content/drive/MyDrive/vivqa_on_book/pretrained_VQA/src/data_preprocessing.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kcJsBUoI35yj"},"outputs":[],"source":["%cd /content/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M1hoFFpyfJTF"},"outputs":[],"source":["dataset = load_dataset(\n","    \"csv\", \n","    data_files={\n","        \"train\": os.path.join(\"data/train.csv\"),\n","        \"test\": os.path.join(\"data/val.csv\")\n","    }\n",")\n","\n","with open(os.path.join(\"data/answer_space.txt\")) as f:\n","    answer_space = f.read().splitlines()\n","\n","dataset = dataset.map(\n","    lambda examples: {\n","        'label': [\n","            answer_space.index(ans.split(\"\\n\")[0]) # Select the 1st answer if multiple answers are provided\n","            for ans in examples['answer']\n","        ]\n","    },\n","    batched=True\n",")\n","\n","dataset"]},{"cell_type":"markdown","metadata":{"id":"rZN8x0YsfJTR"},"source":["### Look at some of the Question/Image/Answer combinations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cYJRRUqPfJTT"},"outputs":[],"source":["from IPython.display import display\n","\n","def showExample(train=True, id=None):\n","    if train:\n","        data = dataset[\"train\"]\n","    else:\n","        data = dataset[\"test\"]\n","    if id == None:\n","        id = np.random.randint(len(data))\n","    image = Image.open(os.path.join(\"./book_cover_img\", \"image_\"+str(data[id][\"image_id\"])+\".jpg\"))\n","    print(\"Question:\\t\", data[id][\"question\"],'?')\n","    print(\"Answer:\\t\\t\", data[id][\"answer\"], \"(Label: {0})\".format(data[id][\"label\"]))\n","    display(image)\n","\n"]},{"cell_type":"code","source":["showExample()"],"metadata":{"id":"0Wptwb7Whmuk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BfMIqYHyfJTW"},"source":["### Create a Multimodal Collator for the Dataset"]},{"cell_type":"markdown","metadata":{"id":"glAd3Dx2fJTX"},"source":["This will be used in the `Trainer()` to automatically create the `Dataloader` from the dataset to pass inputs to the model\n","\n","The collator will process the **question (text)** & the **image**, and return the **tokenized text (with attention masks)** along with the **featurized image** (basically, the **pixel values**). These will be fed into our multimodal transformer model for question answering."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fO8RmgW5fJTY"},"outputs":[],"source":["@dataclass\n","class MultimodalCollator:\n","    tokenizer: AutoTokenizer\n","    preprocessor: AutoFeatureExtractor\n","\n","    def tokenize_text(self, texts: List[str]):\n","        encoded_text = self.tokenizer(\n","            text=texts,\n","            padding='longest',\n","            max_length=50,\n","            truncation=True,\n","            return_tensors='pt',\n","            return_token_type_ids=True,\n","            return_attention_mask=True,\n","        )\n","        return {\n","            \"input_ids\": encoded_text['input_ids'].squeeze(),\n","            \"token_type_ids\": encoded_text['token_type_ids'].squeeze(),\n","            \"attention_mask\": encoded_text['attention_mask'].squeeze(),\n","        }\n","\n","    def preprocess_images(self, images: List[str]):\n","        processed_images = self.preprocessor(\n","            images=[Image.open(os.path.join(\"./book_cover_img/\", \"image_\"+str(image_id)+\".jpg\")).convert('RGB') for image_id in images],\n","            return_tensors=\"pt\",\n","        )\n","        return {\n","            \"pixel_values\": processed_images['pixel_values'].squeeze(),\n","        }\n","            \n","    def __call__(self, raw_batch_dict):\n","        return {\n","            **self.tokenize_text(\n","                raw_batch_dict['question']\n","                if isinstance(raw_batch_dict, dict) else\n","                [i['question'] for i in raw_batch_dict]\n","            ),\n","            **self.preprocess_images(\n","                raw_batch_dict['image_id']\n","                if isinstance(raw_batch_dict, dict) else\n","                [i['image_id'] for i in raw_batch_dict]\n","            ),\n","            'labels': torch.tensor(\n","                raw_batch_dict['label']\n","                if isinstance(raw_batch_dict, dict) else\n","                [i['label'] for i in raw_batch_dict],\n","                dtype=torch.int64\n","            ),\n","        }"]},{"cell_type":"markdown","metadata":{"id":"mP2PCZ0rfJTY"},"source":["## Defining the Multimodal VQA Model Architecture"]},{"cell_type":"markdown","metadata":{"id":"s5VqDTG8fJTY"},"source":["Multimodal models can be of various forms to capture information from the text & image modalities, along with some cross-modal interaction as well.\n","Here, we explore **\"Fusion\" Models**, that fuse information from the text encoder & image encoder to perform the downstream task (visual question answering). \n","\n","The text encoder can be a text-based transformer model (like BERT, RoBERTa, etc.) while the image encoder could be an image transformer (like ViT, Deit, BeIT, etc.). After passing the tokenized question through the text-based transformer & the image features through the image transformer, the outputs are concatenated & passed through a fully-connected network with an output having the same dimensions as the answer-space.\n","\n","Since we model the VQA task as a multi-class classification, it is natural to use the *Cross-Entropy Loss* as the loss function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lOE4WtIPfJTZ"},"outputs":[],"source":["class MultimodalVQAModel(nn.Module):\n","    def __init__(\n","            self,\n","            num_labels: int = len(answer_space),\n","            intermediate_dim: int = 512,\n","            pretrained_text_name: str = 'bert-base-multilingual-uncased',\n","            pretrained_image_name: str = 'microsoft/beit-base-patch16-224-pt22k-ft22k'):\n","     \n","        super(MultimodalVQAModel, self).__init__()\n","        self.num_labels = num_labels\n","        self.pretrained_text_name = pretrained_text_name\n","        self.pretrained_image_name = pretrained_image_name\n","        \n","        self.text_encoder = AutoModel.from_pretrained(\n","            self.pretrained_text_name,\n","        )\n","        self.image_encoder = AutoModel.from_pretrained(\n","            self.pretrained_image_name,\n","        )\n","        self.fusion = nn.Sequential(\n","            nn.Linear(self.text_encoder.config.hidden_size + self.image_encoder.config.hidden_size, intermediate_dim),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","        )\n","        \n","        self.classifier = nn.LogSoftmax(intermediate_dim, self.num_labels)\n","        \n","        self.criterion = nn.CrossEntropyLoss()\n","    \n","    def forward(\n","            self,\n","            input_ids: torch.LongTensor,\n","            pixel_values: torch.FloatTensor,\n","            attention_mask: Optional[torch.LongTensor] = None,\n","            token_type_ids: Optional[torch.LongTensor] = None,\n","            labels: Optional[torch.LongTensor] = None):\n","        \n","        encoded_text = self.text_encoder(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            return_dict=True,\n","        )\n","        encoded_image = self.image_encoder(\n","            pixel_values=pixel_values,\n","            return_dict=True,\n","        )\n","        fused_output = self.fusion(\n","            torch.cat(\n","                [\n","                    encoded_text['pooler_output'],\n","                    encoded_image['pooler_output'],\n","                ],\n","                dim=1\n","            )\n","        )\n","        logits = self.classifier(fused_output)\n","        \n","        out = {\n","            \"logits\": logits\n","        }\n","        if labels is not None:\n","            loss = self.criterion(logits, labels)\n","            out[\"loss\"] = loss\n","        \n","        return out"]},{"cell_type":"markdown","metadata":{"id":"ZEIiiU2VfJTa"},"source":["### Define a Function to Create the Multimodal VQA Models along with their Collators"]},{"cell_type":"markdown","metadata":{"id":"ubTfc1tkfJTa"},"source":["We plan to experiment with multiple pretrained text & image encoders for our VQA Model. Thus, we will have to create the corresponding collators along with the model (tokenizers, featurizers & models need to be loaded from same pretrained checkpoints)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_GoiAqHbfJTb"},"outputs":[],"source":["def createMultimodalVQACollatorAndModel(text='bert-base-multilingual-uncased', image='microsoft/beit-base-patch16-224-pt22k-ft22k'):\n","    tokenizer = AutoTokenizer.from_pretrained(text)\n","    preprocessor = AutoFeatureExtractor.from_pretrained(image)\n","\n","    multi_collator = MultimodalCollator(\n","        tokenizer=tokenizer,\n","        preprocessor=preprocessor,\n","    )\n","\n","\n","    multi_model = MultimodalVQAModel(pretrained_text_name=text, pretrained_image_name=image).to(device)\n","    return multi_collator, multi_model"]},{"cell_type":"markdown","metadata":{"id":"l_h_gp3NfJTb"},"source":["## Performance Metrics from Visual Question Answering"]},{"cell_type":"markdown","metadata":{"id":"XIawjvmEfJTc"},"source":["### Wu and Palmer Similarity\n","\n","The Wu & Palmer similarity is a metric to calculate the sematic similarity between 2 words/phrases based on the position of concepts $c_1$ and $c_2$ in the taxonomy, relative to the position of their **_Least Common Subsumer_** $LCS(c_1, c_2)$. *(In an directed acyclic graph, the Least Common Subsumer is the is the deepest node that has both the nodes under consideration as descendants, where we define each node to be a descendant of itself)*\n","\n","WUP similarity works for single-word answers (& hence, we use if for our task), but doesn't work for phrases or sentences.\n","\n","`nltk` has an implementation of Wu & Palmer similarity score based on the WordNet taxanomy. Here, we have adapted the [implementation of Wu & Palmer similarity as defined along with the DAQUAR dataset](https://datasets.d2.mpi-inf.mpg.de/mateusz14visual-turing/calculate_wups.py).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i9yHZFYjfJTc"},"outputs":[],"source":["def wup_measure(a,b,similarity_threshold=0.8):\n","    \"\"\"\n","    Returns Wu-Palmer similarity score.\n","    More specifically, it computes:\n","        max_{x \\in interp(a)} max_{y \\in interp(b)} wup(x,y)\n","        where interp is a 'interpretation field'\n","    \"\"\"\n","    def get_semantic_field(a):\n","        weight = 1.0\n","        semantic_field = wordnet.synsets(a,pos=wordnet.NOUN)\n","        return (semantic_field,weight)\n","\n","\n","    def get_stem_word(a):\n","        \"\"\"\n","        Sometimes answer has form word\\d+:wordid.\n","        If so we return word and downweight\n","        \"\"\"\n","        weight = 1.0\n","        return (a,weight)\n","\n","\n","    global_weight=1.0\n","\n","    (a,global_weight_a)=get_stem_word(a)\n","    (b,global_weight_b)=get_stem_word(b)\n","    global_weight = min(global_weight_a,global_weight_b)\n","\n","    if a==b:\n","        # they are the same\n","        return 1.0*global_weight\n","\n","    if a==[] or b==[]:\n","        return 0\n","\n","\n","    interp_a,weight_a = get_semantic_field(a) \n","    interp_b,weight_b = get_semantic_field(b)\n","\n","    if interp_a == [] or interp_b == []:\n","        return 0\n","\n","    # we take the most optimistic interpretation\n","    global_max=0.0\n","    for x in interp_a:\n","        for y in interp_b:\n","            local_score=x.wup_similarity(y)\n","            if local_score > global_max:\n","                global_max=local_score\n","\n","    # we need to use the semantic fields and therefore we downweight\n","    # unless the score is high which indicates both are synonyms\n","    if global_max < similarity_threshold:\n","        interp_weight = 0.1\n","    else:\n","        interp_weight = 1.0\n","\n","    final_score=global_max*weight_a*weight_b*interp_weight*global_weight\n","    return final_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vmc0PFRFfJTd"},"outputs":[],"source":["def batch_wup_measure(labels, preds):\n","    wup_scores = [wup_measure(answer_space[label], answer_space[pred]) for label, pred in zip(labels, preds)]\n","    return np.mean(wup_scores)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EVcFvP9DLGZL"},"outputs":[],"source":["import nltk\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-_q-aBWJfJTd"},"outputs":[],"source":["labels = np.random.randint(len(answer_space), size=5)\n","preds = np.random.randint(len(answer_space), size=5)\n","\n","def showAnswers(ids):\n","    print([answer_space[id] for id in ids])\n","\n","showAnswers(labels)\n","showAnswers(preds)\n","\n","print(\"Predictions vs Labels: \", batch_wup_measure(labels, preds))\n","print(\"Labels vs Labels: \", batch_wup_measure(labels, labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pmGgSGj0fJTd"},"outputs":[],"source":["def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n","    logits, labels = eval_tuple\n","    preds = logits.argmax(axis=-1)\n","    return {\n","        \"wups\": batch_wup_measure(labels, preds),\n","        \"acc\": accuracy_score(labels, preds),\n","        \"f1\": f1_score(labels, preds, average='macro')\n","    }"]},{"cell_type":"markdown","metadata":{"id":"aSpS01TUfJTe"},"source":["## Model Training & Evaluation"]},{"cell_type":"markdown","metadata":{"id":"YnZScqRHfJTe"},"source":["### Define the Arguments needed for Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hz_K61Z3fJTe"},"outputs":[],"source":["args = TrainingArguments(\n","    output_dir=\"checkpoint\",\n","    seed=12345, \n","    evaluation_strategy=\"steps\",\n","    eval_steps=100,\n","    logging_strategy=\"steps\",\n","    logging_steps=100,\n","    save_strategy=\"steps\",\n","    save_steps=100,\n","    save_total_limit=3,             # Save only the last 3 checkpoints at any given time while training \n","    metric_for_best_model='acc',\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    remove_unused_columns=False,\n","    num_train_epochs=50,\n","    fp16=True,\n","    # warmup_ratio=0.01,\n","    learning_rate=0.05,\n","    # weight_decay=1e-4,\n","    # gradient_accumulation_steps=2,\n","    dataloader_num_workers=2,\n","    load_best_model_at_end=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"XrWK6ozvfJTf"},"source":["### Create the Multimodal Models using User-Defined Text/Image  Transformers & Train it on the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljc-Zj-pfJTf"},"outputs":[],"source":["def createAndTrainModel(dataset, args, text_model='bert-base-multilingual-uncased', image_model='microsoft/beit-base-patch16-224-pt22k-ft22k', multimodal_model='mbert_beit'):\n","    collator, model = createMultimodalVQACollatorAndModel(text_model, image_model)\n","    \n","    multi_args = deepcopy(args)\n","    multi_args.output_dir = os.path.join(\"/content/checkpoint\", multimodal_model)\n","    multi_trainer = Trainer(\n","        model,\n","        multi_args,\n","        train_dataset=dataset['train'],\n","        eval_dataset=dataset['test'],\n","        data_collator=collator,\n","        compute_metrics=compute_metrics\n","    )\n","    \n","    train_multi_metrics = multi_trainer.train()\n","    eval_multi_metrics = multi_trainer.evaluate()\n","    \n","    return collator, model, train_multi_metrics, eval_multi_metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tS8HxUdAfJTf","tags":[]},"outputs":[],"source":["collator, model, train_multi_metrics, eval_multi_metrics = createAndTrainModel(dataset, args)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PWXScG12fJTf"},"outputs":[],"source":["eval_multi_metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8hyHmanRjgyN"},"outputs":[],"source":["!zip -r /content/checkpoint.zip /content/checkpoint"]},{"cell_type":"markdown","metadata":{"id":"QaQEbN1KfJTf"},"source":["## Examples of Model Inferencing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5RL0BcLllOUe"},"outputs":[],"source":["!mv /content/checkpoint.zip /content/drive/MyDrive/"]},{"cell_type":"markdown","metadata":{"id":"DC34W3uTfJTg"},"source":["### Loading the Model from Checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"8Dd5UFiafJTg","jupyter":{"outputs_hidden":true},"tags":[]},"outputs":[],"source":["model = MultimodalVQAModel()\n","\n","# We use the checkpoint giving best results\n","model.load_state_dict(torch.load(os.path.join(\"/content/checkpoint/bert_beit/checkpoint-1100\", \"pytorch_model.bin\")))\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jd0AxwDXfJTg"},"outputs":[],"source":["sample = collator(dataset[\"test\"][500:510])\n","\n","input_ids = sample[\"input_ids\"].to(device)\n","token_type_ids = sample[\"token_type_ids\"].to(device)\n","attention_mask = sample[\"attention_mask\"].to(device)\n","pixel_values = sample[\"pixel_values\"].to(device)\n","labels = sample[\"labels\"].to(device)"]},{"cell_type":"markdown","metadata":{"id":"axXgB-bwfJTh"},"source":["### Pass the Samples through the Model & inspect the Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_22FuyBfJTh"},"outputs":[],"source":["model.eval()\n","output = model(input_ids, pixel_values, attention_mask, token_type_ids, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ruelrIgvfJTh"},"outputs":[],"source":["preds = output[\"logits\"].argmax(axis=-1).cpu().numpy()\n","preds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iK3dGSgZfJTi"},"outputs":[],"source":["for i in range(500, 510):\n","    print(\"*********************************************************\")\n","    showExample(train=False, id=i)\n","    print(\"Predicted Answer:\\t\", answer_space[preds[i-500]])\n","    print(\"*********************************************************\")"]},{"cell_type":"markdown","metadata":{"id":"d0LcqmDEfJTi"},"source":["## Inspecting Model Size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r0IPHytMfJTj"},"outputs":[],"source":["def countTrainableParameters(model):\n","    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(\"No. of trainable parameters:\\t{0:,}\".format(num_params))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kp3FBqM7fJTj"},"outputs":[],"source":["countTrainableParameters(model) # For BERT-beiT model"]}],"metadata":{"accelerator":"GPU","colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"b9ab610a55a5176458df2dff96f548f03c9f71ac1d536db2e7e5f1815100f41c"}}},"nbformat":4,"nbformat_minor":0}